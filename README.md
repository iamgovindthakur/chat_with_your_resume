# ğŸ“„ Chatâ€‘withâ€‘Yourâ€‘RÃ©sumÃ©

Chat with any PDF rÃ©sumÃ© (or other PDF document) **entirely on your local machine** using:

* **Ollama** â€” runs openâ€‘source LLMs locally (no API keys)
* **LangChain** â€” retrievalâ€‘augmented generation pipeline
* **Streamlit** â€” simple web UI

![Demo Screenshot](static/picture.png)

---

## âœ¨ Features

| Feature                  | Details                                                       |
| ------------------------ | ------------------------------------------------------------- |
| ğŸ—‚ **Upload any PDF**    | Dragâ€‘andâ€‘drop a rÃ©sumÃ© or choose a file.                      |
| ğŸ” **Ask anything**      | Naturalâ€‘language Q\&A powered by your local LLM.              |
| ğŸ§  **RAG pipeline**      | Embeds PDF chunks â†’ Vector DB â†’ LLM answers with citations.\* |
| ğŸ’» **Offline & private** | No data leaves your machine; great for sensitive docs.        |
| ğŸ”„ **Hotâ€‘reload**        | Streamlit autoâ€‘reloads on code changes.                       |

> \* Citations disabled by default; toggle in `rag_chain.py` if needed.

---

## ğŸš€ QuickÂ Start

```bash
# 1. Clone repo & enter it
git clone https://github.com/iamgovindthakur/chat_with_your_resume.git
cd chatâ€‘with-yourâ€‘resume

# 2. Create & activate a virtual env (recommended)
python -m venv .venv
source .venv/bin/activate   # Windows: .venv\Scripts\activate

# 3. Install Python deps
pip install -r requirements.txt

# 4. Make sure Ollama is running & pull a model
ollama serve                 # or launch the GUI app
ollama pull llama3.1:latest   # or any model you prefer

# 5. Run the app ğŸš€
streamlit run app.py
```

Open your browser at **[http://localhost:8501](http://localhost:8501)**.

---

## ğŸ—ï¸  ProjectÂ Structure

```
chat-with-your-resume/
â”œâ”€â”€ app.py                # Streamlit UI
â”œâ”€â”€ rag_chain.py          # LangChain + RAG logic
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
```

---

## âš™ï¸  Configuration

| Setting                  | Where          | Default            | Description                         |
| ------------------------ | -------------- | ------------------ | ----------------------------------- |
| **Model tag**            | `rag_chain.py` | `llama3.1:latest`  | Any model available in Ollama.      |
| **Chunk size / overlap** | `rag_chain.py` | 500 / 50           | Tune for larger or smaller PDFs.    |
| **Embeddings model**     | `rag_chain.py` | Same as chat model | Should match your text model.       |
| **Telemetry**            | `app.py`       | Disabled           | Prevents Chroma telemetry warnings. |

---

## ğŸ§© HowÂ ItÂ Works

1. **Upload PDF** â†’ saved to a temp file.
2. `PyPDFLoader` loads pages â†’ **RecursiveCharacterTextSplitter** breaks into \~500â€‘token chunks.
3. Chunks â†’ **OllamaEmbeddings** â†’ **Chroma** vector store (inâ€‘memory).
4. **RetrievalQA** finds topâ€‘K relevant chunks and feeds them to **ChatOllama**.
5. Response streamed back to Streamlit UI.

---

## ğŸ›   Development Tips

* Use `st.cache_resource` to persist the vector DB across hot reloads.
* Swap models easily with `MODEL_TAG` constant.
* For bigger docs, mount Chroma to disk: `Chroma(persist_directory="./chroma")`.
* Add conversation memory via `ConversationBufferMemory`.

---

## ğŸ› Troubleshooting

| Symptom                                 | Fix                                                                  |
| --------------------------------------- | -------------------------------------------------------------------- |
| Blank Streamlit page                    | Ollama still loading the model â†’ wait; or check terminal for errors. |
| `capture() takes 1 positional argument` | Disable Chroma telemetry: `export ANONYMIZED_TELEMETRY=False`.       |
| `FileNotFoundError: resume.pdf`         | Upload a file or update path in `app.py`.                            |
| `ModuleNotFoundError: psycopg2`         | `pip install psycopg2-binary`.                                       |

---

## ğŸ“œ License

MIT â€” free for personal and commercial use. Attribution appreciated!
